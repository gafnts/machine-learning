{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 11\n",
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.1 Cross-validating models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = datasets.load_digits()\n",
    "features = digits.data\n",
    "target = digits.target\n",
    "\n",
    "standardizer = StandardScaler()\n",
    "logit = LogisticRegression()\n",
    "pipeline = make_pipeline(standardizer, logit)\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "\n",
    "cv_results = cross_val_score(\n",
    "    pipeline,\n",
    "    features, \n",
    "    target,\n",
    "    cv = kf,\n",
    "    scoring = 'accuracy',\n",
    "    n_jobs = -1\n",
    ")\n",
    "\n",
    "cv_results.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train, features_test, target_train, target_test = train_test_split(\n",
    "    features, target, test_size = 0.1, random_state = 1\n",
    ")\n",
    "\n",
    "standardizer.fit(features_train)\n",
    "\n",
    "features_train_std = standardizer.transform(features_train)\n",
    "features_test_std = standardizer.transform(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = make_pipeline(standardizer, logit)\n",
    "\n",
    "cv_results = cross_val_score(\n",
    "    pipeline,\n",
    "    features,\n",
    "    target,\n",
    "    cv = kf,\n",
    "    scoring = 'accuracy',\n",
    "    n_jobs = -1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.2 Creating a baseline regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = load_boston()\n",
    "features, target = boston.data, boston.target\n",
    "features_train, features_test, target_train, target_test = train_test_split(\n",
    "    features, target, random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = DummyRegressor(strategy='mean')\n",
    "dummy.fit(features_train, target_train)\n",
    "dummy.score(features_test, target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "ols = LinearRegression()\n",
    "ols.fit(features_train, target_train)\n",
    "ols.score(features_test, target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DummyRegressor(strategy='constant', constant=20)\n",
    "clf.fit(features_train, target_train)\n",
    "clf.score(features_test, target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.3 Creating a baseline classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "features, target = iris.data, iris.target\n",
    "features_train, features_test, target_train, target_test = train_test_split(\n",
    "    features, target, random_state=0\n",
    ")\n",
    "\n",
    "dummy = DummyClassifier(strategy='uniform', random_state=1)\n",
    "dummy.fit(features_train, target_train)\n",
    "dummy.score(features_test, target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "classifier = RandomForestClassifier()\n",
    "classifier.fit(features_train, target_train)\n",
    "classifier.score(features_test, target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.4 Evaluating binary classifier predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(\n",
    "    n_samples = 10000,\n",
    "    n_features = 3,\n",
    "    n_informative = 3,\n",
    "    n_redundant = 0,\n",
    "    n_classes = 2,\n",
    "    random_state = 1\n",
    ")\n",
    "\n",
    "logit = LogisticRegression()\n",
    "cross_val_score(logit, X, y, scoring = 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(logit, X, y, scoring = 'precision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(logit, X, y, scoring = 'recall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(logit, X, y, scoring = 'f1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.5 Evaluating binary classifier thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, target = make_classification(\n",
    "    n_samples = 10000,\n",
    "    n_features = 10,\n",
    "    n_classes = 2,\n",
    "    n_informative = 3,\n",
    "    random_state = 3\n",
    ")\n",
    "\n",
    "features_train, features_test, target_train, target_test = train_test_split(\n",
    "    features, target, test_size = .1, random_state = 1\n",
    ")\n",
    "\n",
    "logit = LogisticRegression()\n",
    "logit.fit(features_train, target_train)\n",
    "target_probabilities = logit.predict_log_proba(features_test)[:, 1]\n",
    "false_positive_rate, true_positive_rate, threshold = roc_curve(target_test, target_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Receiver operating characteristic')\n",
    "plt.plot(false_positive_rate, true_positive_rate)\n",
    "plt.plot([0, 1], ls = '--')\n",
    "plt.plot([0, 0], [1, 0], c = '.7'), plt.plot([1, 1], c = '.7')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.xlabel('False positive rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Threshold:\", threshold[116])\n",
    "print(\"True Positive Rate:\", true_positive_rate[116]) \n",
    "print(\"False Positive Rate:\", false_positive_rate[116])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Threshold:\", threshold[45])\n",
    "print(\"True Positive Rate:\", true_positive_rate[45]) \n",
    "print(\"False Positive Rate:\", false_positive_rate[45])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(target_test, target_probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.6 Evaluating multiclass classifier predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "features, target = make_classification(\n",
    "    n_samples = 10000,\n",
    "    n_features = 3,\n",
    "    n_informative = 3,\n",
    "    n_redundant = 0,\n",
    "    n_classes = 3,\n",
    "    random_state = 1\n",
    ")\n",
    "\n",
    "logit = LogisticRegression()\n",
    "cross_val_score(logit, features, target, scoring = 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(logit, features, target, scoring = 'f1_macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.7 Visualizing a classifier's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "class_names = iris.target_names\n",
    "\n",
    "features_train, features_test, target_train, target_test = train_test_split(\n",
    "    features, target, random_state = 1\n",
    ")\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "target_predicted = classifier.fit(features_train, target_train).predict(features_test)\n",
    "matrix = confusion_matrix(target_test, target_predicted)\n",
    "\n",
    "dataframe = pd.DataFrame(matrix, index = class_names, columns = class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(dataframe, annot=True, cbar=None, cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix\"), plt.tight_layout()\n",
    "plt.ylabel(\"True Class\"), plt.xlabel(\"Predicted Class\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.8 Evaluating regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import cross_val_score \n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, target = make_regression(\n",
    "    n_samples = 100,\n",
    "    n_features = 3,\n",
    "    n_informative = 3,\n",
    "    n_targets = 1,\n",
    "    noise = 50,\n",
    "    coef = False,\n",
    "    random_state = 1\n",
    ")\n",
    "\n",
    "ols = LinearRegression()\n",
    "print(cross_val_score(ols, features, target, scoring = 'neg_mean_squared_error', cv = 10))\n",
    "print(cross_val_score(ols, features, target, scoring = 'r2', cv = 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.9 Evaluating clustering models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import silhouette_score \n",
    "from sklearn import datasets\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, _ = make_blobs(\n",
    "    n_samples = 1000,\n",
    "    n_features = 10,\n",
    "    centers = 2,\n",
    "    cluster_std = 0.5,\n",
    "    shuffle = True,\n",
    "    random_state = 1\n",
    ")\n",
    "\n",
    "model = KMeans(n_clusters = 2, random_state = 1).fit(features)\n",
    "target_predicted = model.labels_\n",
    "silhouette_score(features, target_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.10 Creating a custom evaluation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer, r2_score \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.datasets import make_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, target = make_regression(\n",
    "    n_samples = 100,\n",
    "    n_features = 3,\n",
    "    random_state = 1\n",
    ")\n",
    "\n",
    "features_train, features_test, target_train, target_test = train_test_split(\n",
    "    features, target, test_size = 0.1, random_state = 1\n",
    ")\n",
    "\n",
    "def custom_metric(target_test, target_predicted): \n",
    "    r2 = r2_score(target_test, target_predicted) \n",
    "    return r2\n",
    "\n",
    "score = make_scorer(custom_metric, greater_is_better = True) \n",
    "\n",
    "classifier = Ridge()\n",
    "model = classifier.fit(features_train, target_train) \n",
    "\n",
    "score(model, features_test, target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.11 Visualizing the effect of training set size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import learning_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()\n",
    "features, target = digits.data, digits.target\n",
    "\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    RandomForestClassifier(),\n",
    "    features,\n",
    "    target,\n",
    "    cv = 10,\n",
    "    scoring = 'accuracy',\n",
    "    n_jobs = -1,\n",
    "    train_sizes = np.linspace(\n",
    "        0.01,\n",
    "        0.1,\n",
    "        50\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "\n",
    "\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_sizes, train_mean, '--', color=\"#111111\",  label=\"Training score\")\n",
    "plt.plot(train_sizes, test_mean, color=\"#111111\", label=\"Cross-validation score\")\n",
    "\n",
    "plt.fill_between(train_sizes, train_mean - train_std,\n",
    "                 train_mean + train_std, color=\"#DDDDDD\")\n",
    "plt.fill_between(train_sizes, test_mean - test_std,\n",
    "                 test_mean + test_std, color=\"#DDDDDD\")\n",
    "\n",
    "plt.title(\"Learning Curve\")\n",
    "plt.xlabel(\"Training Set Size\"), plt.ylabel(\"Accuracy Score\"),\n",
    "plt.legend(loc=\"best\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.12 Creating a text report of evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "features, target, class_names = iris.data, iris.target, iris.target_names\n",
    "\n",
    "features_train, features_test, target_train, target_test = train_test_split(\n",
    "    features, target, random_state = 1\n",
    ")\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "\n",
    "model = classifier.fit(features_train, target_train)\n",
    "target_predicted = model.predict(features_test)\n",
    "\n",
    "print(\n",
    "    classification_report(\n",
    "        target_test, \n",
    "        target_predicted,\n",
    "        target_names = class_names\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.13 Visualizing the effect of hyperparameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.model_selection import validation_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()\n",
    "features, target = digits.data, digits.target\n",
    "\n",
    "param_range = np.arange(1, 250, 2)\n",
    "\n",
    "train_scores, test_scores = validation_curve(\n",
    "    RandomForestClassifier(),\n",
    "    features,\n",
    "    target,\n",
    "    param_name = 'n_estimators',\n",
    "    param_range = param_range,\n",
    "    cv = 3,\n",
    "    scoring = 'accuracy',\n",
    "    n_jobs = -1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(param_range, train_mean, label=\"Training score\", color=\"black\")\n",
    "plt.plot(param_range, test_mean, label=\"Cross-validation score\", color=\"dimgrey\")\n",
    "\n",
    "plt.fill_between(param_range, train_mean - train_std,\n",
    "                 train_mean + train_std, color=\"gray\")\n",
    "plt.fill_between(param_range, test_mean - test_std,\n",
    "                 test_mean + test_std, color=\"gainsboro\")\n",
    "\n",
    "plt.title(\"Validation Curve With Random Forest\")\n",
    "plt.xlabel(\"Number Of Trees\")\n",
    "plt.ylabel(\"Accuracy Score\")\n",
    "plt.tight_layout()\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('ml39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0c01ea7d99fbbbbda0ae4ef04938469e1a67b14c2d49f0fc0f5a9b425216b639"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
