{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 20\n",
    "\n",
    "## Neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20.1 Preprocessing data for neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.array(\n",
    "    [[-100.1, 3240.1],\n",
    "    [-200.2, -234.1],\n",
    "    [5000.5, 150.1],\n",
    "    [6000.6, -125.1],\n",
    "    [9000.9, -673.1]]\n",
    ")\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "features_standardized = scaler.fit_transform(features)\n",
    "features_standardized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f'Mean: {features_standardized[:, 0].mean()}',\n",
    "    f'Standard deviation: {features_standardized[:, 0].std()}',\n",
    "    sep = '\\n'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20.2 Designing a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models \n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = models.Sequential()\n",
    "network.add(layers.Dense(units=16, activation=\"relu\", input_shape=(10,))) \n",
    "network.add(layers.Dense(units=16, activation=\"relu\"))\n",
    "network.add(layers.Dense(units=1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=\"rmsprop\",\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20.3 Training a binary classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.text import Tokenizer \n",
    "from keras import models\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "number_of_features = 1000\n",
    "\n",
    "(data_train, target_train), (data_test, target_test) = imdb.load_data(\n",
    "    num_words=number_of_features\n",
    ")\n",
    "\n",
    "tokenizer = Tokenizer(num_words=number_of_features)\n",
    "features_train = tokenizer.sequences_to_matrix(data_train, mode=\"binary\")\n",
    "features_test = tokenizer.sequences_to_matrix(data_test, mode=\"binary\")\n",
    "\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(units=16, activation=\"relu\", input_shape=(number_of_features,)))\n",
    "network.add(layers.Dense(units=16, activation=\"relu\"))\n",
    "network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "network.compile(\n",
    "    loss=\"binary_crossentropy\", \n",
    "    optimizer=\"rmsprop\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = network.fit(\n",
    "    features_train, \n",
    "    target_train, \n",
    "    epochs=3, \n",
    "    verbose=2,\n",
    "    batch_size=100,\n",
    "    validation_data=(features_test, target_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20.4 Training a multiclass classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import reuters\n",
    "from keras.utils.np_utils import to_categorical \n",
    "from keras.preprocessing.text import Tokenizer \n",
    "from keras import models\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "number_of_features = 5000\n",
    "\n",
    "data = reuters.load_data(num_words=number_of_features)\n",
    "(data_train, target_vector_train), (data_test, target_vector_test) = data\n",
    "\n",
    "tokenizer = Tokenizer(num_words=number_of_features)\n",
    "features_train = tokenizer.sequences_to_matrix(data_train, mode=\"binary\")\n",
    "features_test = tokenizer.sequences_to_matrix(data_test, mode=\"binary\")\n",
    "\n",
    "target_train = to_categorical(target_vector_train)\n",
    "target_test = to_categorical(target_vector_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = models.Sequential()\n",
    "network.add(layers.Dense(units=100, activation=\"relu\", input_shape=(number_of_features,)))\n",
    "network.add(layers.Dense(units=100, activation=\"relu\"))\n",
    "network.add(layers.Dense(units=46, activation=\"softmax\"))\n",
    "\n",
    "network.compile(\n",
    "    loss=\"categorical_crossentropy\", \n",
    "    optimizer=\"rmsprop\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "history = network.fit(\n",
    "    features_train, \n",
    "    target_train, \n",
    "    epochs=3, \n",
    "    verbose=1, \n",
    "    batch_size=100,\n",
    "    validation_data=(features_test, target_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object = features_train\n",
    "\n",
    "print(\n",
    "    f'Shape: {object.shape}',\n",
    "    f'Dimension: {object.ndim}',\n",
    "    f'Size: {object.size}',\n",
    "    sep = '\\n'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20.5 Training a regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "features, target = make_regression(\n",
    "    n_samples = 10000,\n",
    "    n_features = 3,\n",
    "    n_informative = 3,\n",
    "    n_targets = 1,\n",
    "    noise = 0.0,\n",
    "    random_state = 0\n",
    ")\n",
    "\n",
    "features_train, features_test, target_train, target_test = train_test_split(\n",
    "    features, target, test_size=0.33, random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = models.Sequential()\n",
    "network.add(layers.Dense(units=32, activation=\"relu\", input_shape=(features_train.shape[1],)))\n",
    "network.add(layers.Dense(units=32, activation=\"relu\"))\n",
    "network.add(layers.Dense(units=1))\n",
    "\n",
    "network.compile(\n",
    "    loss='mse',\n",
    "    optimizer='RMSprop',\n",
    "    metrics=['mse']\n",
    ")\n",
    "\n",
    "history = network.fit(\n",
    "    features_train,\n",
    "    target_train,\n",
    "    epochs=10,\n",
    "    verbose=1,\n",
    "    batch_size=100,\n",
    "    validation_data=(features_test, target_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20.6 Making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.text import Tokenizer \n",
    "from keras import models\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "number_of_features = 10000\n",
    "\n",
    "(data_train, target_train), (data_test, target_test) = imdb.load_data(\n",
    "    num_words=number_of_features\n",
    ")\n",
    "\n",
    "tokenizer = Tokenizer(num_words=number_of_features)\n",
    "features_train = tokenizer.sequences_to_matrix(data_train, mode=\"binary\")\n",
    "features_test = tokenizer.sequences_to_matrix(data_test, mode=\"binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = models.Sequential()\n",
    "network.add(layers.Dense(units=16, activation=\"relu\", input_shape=(number_of_features,)))\n",
    "network.add(layers.Dense(units=16, activation=\"relu\"))\n",
    "network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "network.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=\"rmsprop\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "history = network.fit(\n",
    "    features_train,\n",
    "    target_train,\n",
    "    epochs=3,\n",
    "    verbose=0,\n",
    "    batch_size=100, \n",
    "    validation_data=(features_test, target_test)\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_target = network.predict(features_test)\n",
    "predicted_target[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20.7 Visualize training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.text import Tokenizer \n",
    "from keras import models\n",
    "from keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set_theme(style=\"darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "number_of_features = 10000\n",
    "\n",
    "(data_train, target_train), (data_test, target_test) = imdb.load_data(\n",
    "    num_words=number_of_features\n",
    ")\n",
    "\n",
    "tokenizer = Tokenizer(num_words=number_of_features)\n",
    "features_train = tokenizer.sequences_to_matrix(data_train, mode=\"binary\")\n",
    "features_test = tokenizer.sequences_to_matrix(data_test, mode=\"binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = models.Sequential()\n",
    "network.add(layers.Dense(units=16, activation=\"relu\", input_shape=(number_of_features,)))\n",
    "network.add(layers.Dense(units=16, activation=\"relu\"))\n",
    "network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "network.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=\"rmsprop\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "history = network.fit(\n",
    "    features_train,\n",
    "    target_train,\n",
    "    epochs=15,\n",
    "    verbose=0,\n",
    "    batch_size=1000, \n",
    "    validation_data=(features_test, target_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loss = history.history[\"loss\"]\n",
    "test_loss = history.history[\"val_loss\"]\n",
    "epoch_count = range(1, len(training_loss) + 1)\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(epoch_count, training_loss, \"r--\")\n",
    "plt.plot(epoch_count, test_loss, \"b-\")\n",
    "plt.legend([\"Training Loss\", \"Test Loss\"])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(\n",
    "    list(zip(training_loss, test_loss, epoch_count)),\n",
    "    columns=['training_loss', 'test_loss', 'epoch_count']\n",
    ")\n",
    "\n",
    "data_long = pd.melt(\n",
    "    frame=data,\n",
    "    id_vars='epoch_count',\n",
    "    value_vars=['training_loss', 'test_loss']\n",
    ")\n",
    "\n",
    "data_long.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={\"figure.dpi\":300})\n",
    "\n",
    "sns.relplot(\n",
    "    data=data_long,\n",
    "    x='epoch_count',\n",
    "    y='value',\n",
    "    hue='variable',\n",
    "    aspect = 40/20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_accuracy = history.history[\"accuracy\"]\n",
    "test_accuracy = history.history[\"val_accuracy\"]\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(epoch_count, training_accuracy, \"r--\")\n",
    "plt.plot(epoch_count, test_accuracy, \"b-\")\n",
    "plt.legend([\"Training Accuracy\", \"Test Accuracy\"])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy Score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20.8 Reducing overfitting with weight regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.text import Tokenizer \n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "number_of_features = 1000\n",
    "\n",
    "(data_train, target_train), (data_test, target_test) = imdb.load_data(\n",
    "    num_words=number_of_features\n",
    ")\n",
    "\n",
    "tokenizer = Tokenizer(num_words=number_of_features)\n",
    "features_train = tokenizer.sequences_to_matrix(data_train, mode=\"binary\")\n",
    "features_test = tokenizer.sequences_to_matrix(data_test, mode=\"binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = models.Sequential()\n",
    "\n",
    "network.add(layers.Dense(units=16,\n",
    "                         activation=\"relu\",\n",
    "                         kernel_regularizer=regularizers.l2(0.01),\n",
    "                         input_shape=(number_of_features,)))\n",
    "\n",
    "network.add(layers.Dense(units=16,\n",
    "                         kernel_regularizer=regularizers.l2(0.01),\n",
    "                         activation=\"relu\"))\n",
    "\n",
    "network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "network.compile(\n",
    "    loss=\"binary_crossentropy\", \n",
    "    optimizer=\"rmsprop\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "history = network.fit(\n",
    "    features_train,\n",
    "    target_train,\n",
    "    epochs=15,\n",
    "    verbose=0,\n",
    "    batch_size=1000,\n",
    "    validation_data=(features_test, target_test)\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_loss(training_loss, test_loss, epoch_count):\n",
    "    data = pd.DataFrame(\n",
    "        list(zip(training_loss, test_loss, epoch_count)),\n",
    "        columns=['training_loss', 'test_loss', 'epoch_count']\n",
    "    )\n",
    "\n",
    "    data_long = pd.melt(\n",
    "        frame=data,\n",
    "        id_vars='epoch_count',\n",
    "        value_vars=['training_loss', 'test_loss']\n",
    "    )\n",
    "\n",
    "    sns.set(rc={\"figure.dpi\":300})\n",
    "\n",
    "    sns.relplot(\n",
    "        data=data_long,\n",
    "        x='epoch_count',\n",
    "        y='value',\n",
    "        hue='variable',\n",
    "        aspect = 40/20\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loss = history.history[\"loss\"]\n",
    "test_loss = history.history[\"val_loss\"]\n",
    "epoch_count = range(1, len(training_loss) + 1)\n",
    "\n",
    "plot_training_loss(training_loss, test_loss, epoch_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20.9 Reducing overfitting with early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "sns.set_theme(style=\"darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "number_of_features = 1000\n",
    "\n",
    "(data_train, target_train), (data_test, target_test) = imdb.load_data(\n",
    "    num_words=number_of_features\n",
    ")\n",
    "\n",
    "tokenizer = Tokenizer(num_words=number_of_features)\n",
    "features_train = tokenizer.sequences_to_matrix(data_train, mode=\"binary\")\n",
    "features_test = tokenizer.sequences_to_matrix(data_test, mode=\"binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = models.Sequential()\n",
    "network.add(layers.Dense(units=16,\n",
    "                         activation=\"relu\",\n",
    "                         input_shape=(number_of_features,)))\n",
    "network.add(layers.Dense(units=16, activation=\"relu\"))\n",
    "network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "network.compile(\n",
    "    loss=\"binary_crossentropy\", \n",
    "    optimizer=\"rmsprop\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor=\"val_loss\", patience=2),\n",
    "    ModelCheckpoint(filepath=\"best_model.h5\",\n",
    "    monitor=\"val_loss\",\n",
    "    save_best_only=True)\n",
    "]\n",
    "\n",
    "history = network.fit(\n",
    "    features_train,\n",
    "    target_train,\n",
    "    epochs=20,\n",
    "    verbose=0,\n",
    "    batch_size=100,\n",
    "    validation_data=(features_test, target_test)\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loss = history.history[\"loss\"]\n",
    "test_loss = history.history[\"val_loss\"]\n",
    "epoch_count = range(1, len(training_loss) + 1)\n",
    "\n",
    "plot_training_loss(training_loss, test_loss, epoch_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20.10 Reducing overfitting with dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.text import Tokenizer \n",
    "from keras import models\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "number_of_features = 1000\n",
    "\n",
    "(data_train, target_train), (data_test, target_test) = imdb.load_data(\n",
    "    num_words=number_of_features\n",
    ")\n",
    "\n",
    "tokenizer = Tokenizer(num_words=number_of_features)\n",
    "features_train = tokenizer.sequences_to_matrix(data_train, mode=\"binary\")\n",
    "features_test = tokenizer.sequences_to_matrix(data_test, mode=\"binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = models.Sequential()\n",
    "network.add(layers.Dropout(0.2, input_shape=(number_of_features,)))\n",
    "network.add(layers.Dense(units=16, activation=\"relu\"))\n",
    "network.add(layers.Dropout(0.5))\n",
    "network.add(layers.Dense(units=16, activation=\"relu\")) \n",
    "network.add(layers.Dropout(0.5))\n",
    "network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "network.compile(\n",
    "    loss=\"binary_crossentropy\", \n",
    "    optimizer=\"rmsprop\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "history = network.fit(\n",
    "    features_train,\n",
    "    target_train,\n",
    "    epochs=15,\n",
    "    verbose=0,\n",
    "    batch_size=1000,\n",
    "    validation_data=(features_test, target_test)\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loss = history.history[\"loss\"]\n",
    "test_loss = history.history[\"val_loss\"]\n",
    "epoch_count = range(1, len(training_loss) + 1)\n",
    "\n",
    "plot_training_loss(training_loss, test_loss, epoch_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20.11 Saving model training progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.text import Tokenizer \n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "number_of_features = 1000\n",
    "\n",
    "(data_train, target_train), (data_test, target_test) = imdb.load_data(\n",
    "    num_words=number_of_features\n",
    ")\n",
    "\n",
    "tokenizer = Tokenizer(num_words=number_of_features)\n",
    "features_train = tokenizer.sequences_to_matrix(data_train, mode=\"binary\")\n",
    "features_test = tokenizer.sequences_to_matrix(data_test, mode=\"binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = models.Sequential()\n",
    "network.add(layers.Dense(units=16,\n",
    "                         activation=\"relu\",\n",
    "                         input_shape=(number_of_features,)))\n",
    "network.add(layers.Dense(units=16, activation=\"relu\"))\n",
    "network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "network.compile(\n",
    "    loss=\"binary_crossentropy\", \n",
    "    optimizer=\"rmsprop\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "checkpoint = [ModelCheckpoint(filepath=\"models.hdf5\")]\n",
    "\n",
    "history = network.fit(\n",
    "    features_train,\n",
    "    target_train,\n",
    "    epochs=20,\n",
    "    callbacks=checkpoint,\n",
    "    verbose=0,\n",
    "    batch_size=100,\n",
    "    validation_data=(features_test, target_test)\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20.12 k-Fold cross-validating neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score \n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "number_of_features = 100\n",
    "\n",
    "features, target = make_classification(\n",
    "    n_samples = 10000,\n",
    "    n_features = number_of_features,\n",
    "    n_informative = 3,\n",
    "    n_redundant = 0,\n",
    "    n_classes = 2,\n",
    "    weights = [.5, .5],\n",
    "    random_state = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_network():\n",
    "    network = models.Sequential()\n",
    "    network.add(layers.Dense(units=16, activation=\"relu\", input_shape=(number_of_features,)))\n",
    "    network.add(layers.Dense(units=16, activation=\"relu\"))\n",
    "    network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "    network.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=[\"accuracy\"]) \n",
    "    return network\n",
    "\n",
    "neural_network = KerasClassifier(\n",
    "    build_fn=create_network,\n",
    "    epochs=10,\n",
    "    batch_size=100,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "cross_val_score(neural_network, features, target, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(neural_network, features, target, cv=3).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20.13 Tuning neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.wrappers.scikit_learn import KerasClassifier \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "number_of_features = 100\n",
    "\n",
    "features, target = make_classification(\n",
    "    n_samples = 10000,\n",
    "    n_features = number_of_features,\n",
    "    n_informative = 3,\n",
    "    n_redundant = 0,\n",
    "    n_classes = 2,\n",
    "    weights = [.5, .5],\n",
    "    random_state = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_network(optimizer=\"rmsprop\"): \n",
    "    network = models.Sequential()\n",
    "    network.add(layers.Dense(units=16, activation=\"relu\", input_shape=(number_of_features,))) \n",
    "    network.add(layers.Dense(units=16, activation=\"relu\"))\n",
    "    network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "    network.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "    return network\n",
    "\n",
    "neural_network = KerasClassifier(\n",
    "    build_fn=create_network, \n",
    "    verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = [5, 10]\n",
    "batches = [5, 10, 100]\n",
    "optimizers = [\"rmsprop\", \"adam\"]\n",
    "\n",
    "hyperparameters = dict(optimizer=optimizers, epochs=epochs, batch_size=batches)\n",
    "hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(\n",
    "    estimator=neural_network, \n",
    "    param_grid=hyperparameters\n",
    ")\n",
    "\n",
    "grid_result = grid.fit(features, target)\n",
    "grid_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_result.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20.14 Visualizing neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot \n",
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = models.Sequential()\n",
    "network.add(layers.Dense(units=16, activation=\"relu\", input_shape=(10,))) \n",
    "network.add(layers.Dense(units=16, activation=\"relu\"))\n",
    "network.add(layers.Dense(units=1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVG(model_to_dot(network, show_shapes=True).create(prog=\"dot\", format=\"svg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(network, show_shapes=True, to_file=\"network.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20.15 Classifying images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D \n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.set_image_data_format(\"channels_first\")\n",
    "np.random.seed(0)\n",
    "\n",
    "height = 28\n",
    "width = 28\n",
    "channels = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "(data_train, target_train), (data_test, target_test) = mnist.load_data() \n",
    "data_train = data_train.reshape(data_train.shape[0], channels, height, width)\n",
    "data_test = data_test.reshape(data_test.shape[0], channels, height, width)\n",
    "\n",
    "features_train = data_train / 255\n",
    "features_test = data_test / 255\n",
    "\n",
    "target_train = np_utils.to_categorical(target_train)\n",
    "target_test = np_utils.to_categorical(target_test)\n",
    "number_of_classes = target_test.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Sequential()\n",
    "network.add(Conv2D(filters=64,\n",
    "                   kernel_size=(5, 5),\n",
    "                   input_shape=(channels, width, height),\n",
    "                   activation='relu'))\n",
    "network.add(MaxPooling2D(pool_size=(2, 2))) \n",
    "network.add(Dropout(0.5))\n",
    "network.add(Flatten())\n",
    "network.add(Dense(128, activation=\"relu\")) \n",
    "network.add(Dropout(0.5))\n",
    "network.add(Dense(number_of_classes, activation=\"softmax\"))\n",
    "\n",
    "network.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=\"rmsprop\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.fit(\n",
    "    features_train, \n",
    "    target_train, \n",
    "    epochs=2, \n",
    "    verbose=0, \n",
    "    batch_size=1000,\n",
    "    validation_data=(features_test, target_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20.16 Improving Performance with Image Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('ml39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0c01ea7d99fbbbbda0ae4ef04938469e1a67b14c2d49f0fc0f5a9b425216b639"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
