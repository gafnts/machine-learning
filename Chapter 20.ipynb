{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 20\n",
    "\n",
    "## Neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20.1 Preprocessing data for neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.array([[-100.1, 3240.1],\n",
    "                        [-200.2, -234.1],\n",
    "                        [5000.5, 150.1],\n",
    "                        [6000.6, -125.1],\n",
    "                        [9000.9, -673.1]])\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "features_standardized = scaler.fit_transform(features)\n",
    "features_standardized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f'Mean: {features_standardized[:, 0].mean()}',\n",
    "    f'Standard deviation: {features_standardized[:, 0].std()}',\n",
    "    sep = '\\n'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20.2 Designing a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models \n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = models.Sequential()\n",
    "network.add(layers.Dense(units=16, activation=\"relu\", input_shape=(10,))) \n",
    "network.add(layers.Dense(units=16, activation=\"relu\"))\n",
    "network.add(layers.Dense(units=1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=\"rmsprop\",\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20.3 Training a binary classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.text import Tokenizer \n",
    "from keras import models\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "number_of_features = 1000\n",
    "\n",
    "(data_train, target_train), (data_test, target_test) = imdb.load_data(\n",
    "    num_words=number_of_features\n",
    ")\n",
    "\n",
    "tokenizer = Tokenizer(num_words=number_of_features)\n",
    "features_train = tokenizer.sequences_to_matrix(data_train, mode=\"binary\")\n",
    "features_test = tokenizer.sequences_to_matrix(data_test, mode=\"binary\")\n",
    "\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(units=16, activation=\"relu\", input_shape=(number_of_features,)))\n",
    "network.add(layers.Dense(units=16, activation=\"relu\"))\n",
    "network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "network.compile(\n",
    "    loss=\"binary_crossentropy\", \n",
    "    optimizer=\"rmsprop\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = network.fit(\n",
    "    features_train, \n",
    "    target_train, \n",
    "    epochs=3, \n",
    "    verbose=2,\n",
    "    batch_size=100,\n",
    "    validation_data=(features_test, target_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20.4 Training a multiclass classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import reuters\n",
    "from keras.utils.np_utils import to_categorical \n",
    "from keras.preprocessing.text import Tokenizer \n",
    "from keras import models\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "number_of_features = 5000\n",
    "\n",
    "data = reuters.load_data(num_words=number_of_features)\n",
    "(data_train, target_vector_train), (data_test, target_vector_test) = data\n",
    "\n",
    "tokenizer = Tokenizer(num_words=number_of_features)\n",
    "features_train = tokenizer.sequences_to_matrix(data_train, mode=\"binary\")\n",
    "features_test = tokenizer.sequences_to_matrix(data_test, mode=\"binary\")\n",
    "\n",
    "target_train = to_categorical(target_vector_train)\n",
    "target_test = to_categorical(target_vector_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = models.Sequential()\n",
    "network.add(layers.Dense(units=100, activation=\"relu\", input_shape=(number_of_features,)))\n",
    "network.add(layers.Dense(units=100, activation=\"relu\"))\n",
    "network.add(layers.Dense(units=46, activation=\"softmax\"))\n",
    "\n",
    "network.compile(\n",
    "    loss=\"categorical_crossentropy\", \n",
    "    optimizer=\"rmsprop\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "history = network.fit(\n",
    "    features_train, \n",
    "    target_train, \n",
    "    epochs=3, \n",
    "    verbose=1, \n",
    "    batch_size=100,\n",
    "    validation_data=(features_test, target_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object = features_train\n",
    "\n",
    "print(\n",
    "    f'Shape: {object.shape}',\n",
    "    f'Dimension: {object.ndim}',\n",
    "    f'Size: {object.size}',\n",
    "    sep = '\\n'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20.5 Training a regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "features, target = make_regression(\n",
    "    n_samples = 10000,\n",
    "    n_features = 3,\n",
    "    n_informative = 3,\n",
    "    n_targets = 1,\n",
    "    noise = 0.0,\n",
    "    random_state = 0\n",
    ")\n",
    "\n",
    "features_train, features_test, target_train, target_test = train_test_split(\n",
    "    features, target, test_size=0.33, random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = models.Sequential()\n",
    "network.add(layers.Dense(units=32, activation=\"relu\", input_shape=(features_train.shape[1],)))\n",
    "network.add(layers.Dense(units=32, activation=\"relu\"))\n",
    "network.add(layers.Dense(units=1))\n",
    "\n",
    "network.compile(\n",
    "    loss='mse',\n",
    "    optimizer='RMSprop',\n",
    "    metrics=['mse']\n",
    ")\n",
    "\n",
    "history = network.fit(\n",
    "    features_train,\n",
    "    target_train,\n",
    "    epochs=10,\n",
    "    verbose=1,\n",
    "    batch_size=100,\n",
    "    validation_data=(features_test, target_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20.6 Making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.text import Tokenizer \n",
    "from keras import models\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "number_of_features = 10000\n",
    "\n",
    "(data_train, target_train), (data_test, target_test) = imdb.load_data(\n",
    "    num_words=number_of_features\n",
    ")\n",
    "\n",
    "tokenizer = Tokenizer(num_words=number_of_features)\n",
    "features_train = tokenizer.sequences_to_matrix(data_train, mode=\"binary\")\n",
    "features_test = tokenizer.sequences_to_matrix(data_test, mode=\"binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = models.Sequential()\n",
    "network.add(layers.Dense(units=16, activation=\"relu\", input_shape=(number_of_features,)))\n",
    "network.add(layers.Dense(units=16, activation=\"relu\"))\n",
    "network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "network.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=\"rmsprop\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "history = network.fit(\n",
    "    features_train,\n",
    "    target_train,\n",
    "    epochs=3,\n",
    "    verbose=0,\n",
    "    batch_size=100, \n",
    "    validation_data=(features_test, target_test)\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_target = network.predict(features_test)\n",
    "predicted_target[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20.7 Visualize training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.text import Tokenizer \n",
    "from keras import models\n",
    "from keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set_theme(style=\"darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "number_of_features = 10000\n",
    "\n",
    "(data_train, target_train), (data_test, target_test) = imdb.load_data(\n",
    "    num_words=number_of_features\n",
    ")\n",
    "\n",
    "tokenizer = Tokenizer(num_words=number_of_features)\n",
    "features_train = tokenizer.sequences_to_matrix(data_train, mode=\"binary\")\n",
    "features_test = tokenizer.sequences_to_matrix(data_test, mode=\"binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = models.Sequential()\n",
    "network.add(layers.Dense(units=16, activation=\"relu\", input_shape=(number_of_features,)))\n",
    "network.add(layers.Dense(units=16, activation=\"relu\"))\n",
    "network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "network.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=\"rmsprop\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "history = network.fit(\n",
    "    features_train,\n",
    "    target_train,\n",
    "    epochs=15,\n",
    "    verbose=0,\n",
    "    batch_size=1000, \n",
    "    validation_data=(features_test, target_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loss = history.history[\"loss\"]\n",
    "test_loss = history.history[\"val_loss\"]\n",
    "epoch_count = range(1, len(training_loss) + 1)\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(epoch_count, training_loss, \"r--\")\n",
    "plt.plot(epoch_count, test_loss, \"b-\")\n",
    "plt.legend([\"Training Loss\", \"Test Loss\"])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(\n",
    "    list(zip(training_loss, test_loss, epoch_count)),\n",
    "    columns=['training_loss', 'test_loss', 'epoch_count']\n",
    ")\n",
    "\n",
    "data_long = pd.melt(\n",
    "    frame=data,\n",
    "    id_vars='epoch_count',\n",
    "    value_vars=['training_loss', 'test_loss']\n",
    ")\n",
    "\n",
    "data_long.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={\"figure.dpi\":300})\n",
    "\n",
    "sns.relplot(\n",
    "    data=data_long,\n",
    "    x='epoch_count',\n",
    "    y='value',\n",
    "    hue='variable',\n",
    "    aspect = 40/20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_accuracy = history.history[\"accuracy\"]\n",
    "test_accuracy = history.history[\"val_accuracy\"]\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(epoch_count, training_accuracy, \"r--\")\n",
    "plt.plot(epoch_count, test_accuracy, \"b-\")\n",
    "plt.legend([\"Training Accuracy\", \"Test Accuracy\"])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy Score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20.8 Reducing overfitting with weight regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.text import Tokenizer \n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "number_of_features = 1000\n",
    "\n",
    "(data_train, target_train), (data_test, target_test) = imdb.load_data(\n",
    "    num_words=number_of_features\n",
    ")\n",
    "\n",
    "tokenizer = Tokenizer(num_words=number_of_features)\n",
    "features_train = tokenizer.sequences_to_matrix(data_train, mode=\"binary\")\n",
    "features_test = tokenizer.sequences_to_matrix(data_test, mode=\"binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = models.Sequential()\n",
    "\n",
    "network.add(layers.Dense(units=16,\n",
    "                         activation=\"relu\",\n",
    "                         kernel_regularizer=regularizers.l2(0.01),\n",
    "                         input_shape=(number_of_features,)))\n",
    "\n",
    "network.add(layers.Dense(units=16,\n",
    "                         kernel_regularizer=regularizers.l2(0.01),\n",
    "                         activation=\"relu\"))\n",
    "\n",
    "network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "network.compile(\n",
    "    loss=\"binary_crossentropy\", \n",
    "    optimizer=\"rmsprop\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "history = network.fit(\n",
    "    features_train,\n",
    "    target_train,\n",
    "    epochs=15,\n",
    "    verbose=0,\n",
    "    batch_size=1000,\n",
    "    validation_data=(features_test, target_test)\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_loss(training_loss, test_loss, epoch_count):\n",
    "    data = pd.DataFrame(\n",
    "        list(zip(training_loss, test_loss, epoch_count)),\n",
    "        columns=['training_loss', 'test_loss', 'epoch_count']\n",
    "    )\n",
    "\n",
    "    data_long = pd.melt(\n",
    "        frame=data,\n",
    "        id_vars='epoch_count',\n",
    "        value_vars=['training_loss', 'test_loss']\n",
    "    )\n",
    "\n",
    "    sns.set(rc={\"figure.dpi\":300})\n",
    "\n",
    "    sns.relplot(\n",
    "        data=data_long,\n",
    "        x='epoch_count',\n",
    "        y='value',\n",
    "        hue='variable',\n",
    "        aspect = 40/20\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loss = history.history[\"loss\"]\n",
    "test_loss = history.history[\"val_loss\"]\n",
    "epoch_count = range(1, len(training_loss) + 1)\n",
    "\n",
    "plot_training_loss(training_loss, test_loss, epoch_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20.9 Reducing overfitting with early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('ml39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0c01ea7d99fbbbbda0ae4ef04938469e1a67b14c2d49f0fc0f5a9b425216b639"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
